# ğŸ¦™ llama

**Studying LLaMAâ€‘2/LLaMAâ€‘3 architecture by replicating it from scratch.**  
Implemented core components like Rotary Position Embeddings (RoPE), Gated Q-Activation (GQA), SwiGLU, and RMSNorm.

---

## ğŸ“š Table of Contents

1. [Project Overview](#project-overview)  
2. [Features](#features)  
3. [Getting Started](#getting-started)  
4. [Usage](#usage)  
5. [Architecture Details](#architecture-details)  
6. [References](#references)  
7. [Contributing](#contributing)  
8. [License](#license)

---

## Project Overview

This repository is a handsâ€‘on implementation of the LLaMAâ€‘2/LLaMAâ€‘3 model family, optimized for learning and experimentation. Built from the ground up to better understand LLM internalsâ€”including positional encodings, activation functions, and normalization layers.

---

## Features

- ğŸ“ **Rotary Position Embeddings (RoPE)** â€“ for context-aware attention positioning.  
- âš™ï¸ **Gated Q-Activation (GQA)** â€“ improves representational capability.  
- ğŸ”€ **SwiGLU Activation Function** â€“ enhances nonlinearity in feedâ€‘forward layers.  
- ğŸ“ **RMSNorm** â€“ efficient normalization for transformer blocks.  

(This is a learning-focused projectâ€”**not optimized for production**.)

---

## Getting Started

### Requirements

- Python 3.10+  
- PyTorch 2.0+ (or later)  
- NumPy  
